import streamlit as st
from PIL import Image
import torch
from transformers import (
    AutoProcessor,
    AutoModelForCausalLM,
    MarianTokenizer,
    MarianMTModel
)
import logging

# Disable unnecessary logging
logging.getLogger("transformers").setLevel(logging.ERROR)

# Load models (cached for performance)
@st.cache_resource(show_spinner=False)
def load_models():
    # Load VQA model
    processor = AutoProcessor.from_pretrained("Mohamed264/llava-medical-VQA-lora-merged3")
    llava_model = AutoModelForCausalLM.from_pretrained(
        "Mohamed264/llava-medical-VQA-lora-merged3",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # Load translation models
    ar_en_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
    ar_en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
    
    en_ar_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
    en_ar_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
    
    return processor, llava_model, ar_en_tokenizer, ar_en_model, en_ar_tokenizer, en_ar_model

# Translation functions
def translate_ar_to_en(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

def translate_en_to_ar(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# Medical term dictionary
medical_terms = {
    "chest x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ© Ù„Ù„ØµØ¯Ø±",
    "x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©",
    "ct scan": "ØªØµÙˆÙŠØ± Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­ÙˆØ³Ø¨",
    "mri": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ",
    "ultrasound": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ù…ÙˆØ¬Ø§Øª ÙÙˆÙ‚ Ø§Ù„ØµÙˆØªÙŠØ©",
    "normal": "Ø·Ø¨ÙŠØ¹ÙŠ",
    "abnormal": "ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ",
    "brain": "Ø§Ù„Ø¯Ù…Ø§Øº",
    "fracture": "ÙƒØ³Ø±",
    "no abnormality detected": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø´Ø°ÙˆØ°Ø§Øª",
    "left lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠØ³Ø±Ù‰",
    "right lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠÙ…Ù†Ù‰",
    "pneumonia": "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø±Ø¦ÙˆÙŠ",
    "tumor": "ÙˆØ±Ù…",
    "cancer": "Ø³Ø±Ø·Ø§Ù†",
    "infection": "Ø¹Ø¯ÙˆÙ‰"
}

def translate_answer_medical(answer_en, en_ar_tokenizer, en_ar_model):
    key = answer_en.lower().strip()
    if key in medical_terms:
        return medical_terms[key]
    else:
        return translate_en_to_ar(answer_en, en_ar_tokenizer, en_ar_model)

# Main function
def vqa_multilingual(image, question, processor, llava_model, ar_en_tokenizer, ar_en_model, en_ar_tokenizer, en_ar_model):
    # Check if Arabic
    is_arabic = any('\u0600' <= c <= '\u06FF' for c in question)
    
    if is_arabic:
        question_ar = question.strip()
        question_en = translate_ar_to_en(question_ar, ar_en_tokenizer, ar_en_model)
    else:
        question_en = question.strip()
        question_ar = translate_en_to_ar(question_en, en_ar_tokenizer, en_ar_model)
    
    # Process with VQA model
    inputs = processor(text=question_en, images=image, return_tensors="pt").to("cuda")
    with torch.no_grad():
        output = llava_model.generate(**inputs, max_new_tokens=200)
    answer_en = processor.decode(output[0], skip_special_tokens=True).strip()
    
    # Translate answer
    answer_ar = translate_answer_medical(answer_en, en_ar_tokenizer, en_ar_model)
    
    return question_ar, question_en, answer_ar, answer_en

# Streamlit UI
def main():
    st.set_page_config(page_title="Medical VQA Chatbot", layout="wide")
    st.title("ğŸ©º Ù†Ù…ÙˆØ°Ø¬ Ø·Ø¨ÙŠ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø¹Ø±Ø¨ÙŠ - Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)")
    st.write("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© ÙˆØ§Ø³Ø£Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ÙˆØ³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØªÙŠÙ†")
    
    # Initialize models
    with st.spinner("Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚"):
        try:
            processor, llava_model, ar_en_tokenizer, ar_en_model, en_ar_tokenizer, en_ar_model = load_models()
            st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {str(e)}")
            st.stop()
    
    # Create two columns
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ” Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ø´Ø¹Ø©")
        uploaded_file = st.file_uploader("Upload medical image", type=["jpg", "png", "jpeg"], label_visibility="visible")
        
        if uploaded_file:
            try:
                image = Image.open(uploaded_file).convert("RGB")
                st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", use_column_width=True)
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {str(e)}")
    
    with col2:
        st.subheader("ğŸ’¬ Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ")
        question = st.text_area("Enter your question", placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...", height=150, label_visibility="visible")
        
        if st.button("Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", use_container_width=True):
            if not uploaded_file or not question.strip():
                st.warning("ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„")
            else:
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ..."):
                    try:
                        image_pil = Image.open(uploaded_file).convert("RGB")
                        q_ar, q_en, a_ar, a_en = vqa_multilingual(
                            image_pil, question, processor, llava_model, 
                            ar_en_tokenizer, ar_en_model, en_ar_tokenizer, en_ar_model
                        )
                        
                        st.divider()
                        st.subheader("Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
                        
                        st.markdown(f"**ğŸŸ  Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**\n{q_ar}")
                        st.markdown(f"**ğŸŸ¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:**\n{q_en}")
                        st.markdown(f"**ğŸŸ  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**\n{a_ar}")
                        st.markdown(f"**ğŸŸ¢ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:**\n{a_en}")
                        
                    except Exception as e:
                        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
    
    # Footer
    st.divider()
    st.caption("Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø£ÙˆÙ„ÙŠ. Ø§Ø³ØªØ´Ø± Ø·Ø¨ÙŠØ¨Ù‹Ø§ Ù…ØªØ®ØµØµÙ‹Ø§ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚.")

if __name__ == "__main__":
    main()
